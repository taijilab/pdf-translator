from deep_translator import GoogleTranslator
import fitz  # PyMuPDF
import requests
import io
import re
import os
import time

class PDFTranslator:
    # APIä»·æ ¼ï¼ˆæ¯1M tokensçš„ä»·æ ¼ï¼Œå•ä½ï¼šç¾å…ƒï¼‰
    PRICING = {
        'google': {
            'input': 0,
            'output': 0
        },
        'deepseek': {
            'input': 0.14,  # $0.14 per 1M tokens
            'output': 0.28  # $0.28 per 1M tokens
        },
        'zhipu': {
            'input': 0.5,   # GLM-4-Flash: ~$0.5 per 1M tokens
            'output': 0.5
        },
        'openrouter': {
            'input': 0.14,  # DeepSeek via OpenRouter: $0.14 per 1M tokens
            'output': 0.28  # DeepSeek via OpenRouter: $0.28 per 1M tokens
        },
        'kimi': {
            'input': 1.2,   # Moonshot v1-auto via OpenRouter: ~$1.2 per 1M tokens
            'output': 1.2   # Moonshot v1-auto via OpenRouter: ~$1.2 per 1M tokens
        },
        'gpt': {
            'input': 2.0,   # GPT-4.1 via OpenRouter: ~$2.0 per 1M tokens
            'output': 8.0   # GPT-4.1 via OpenRouter: ~$8.0 per 1M tokens
        }
    }

    def __init__(self, api_type='google', api_key=None, progress_callback=None, log_callback=None, cancel_callback=None):
        self.api_type = api_type
        self.api_key = api_key
        self.progress_callback = progress_callback
        self.log_callback = log_callback
        self.cancel_callback = cancel_callback
        self.input_tokens = 0
        self.output_tokens = 0
        self.translator = None  # åˆå§‹åŒ–ä¸ºNone

        # åªåœ¨éœ€è¦æ—¶åˆå§‹åŒ–translator
        if self.api_type == 'google':
            self._setup_translator()

    def _setup_translator(self):
        """è®¾ç½®ç¿»è¯‘å™¨"""
        # deep-translator ä¸éœ€è¦é¢„å…ˆè®¾ç½®translatorå®ä¾‹
        pass

    def analyze_pdf(self, input_path):
        """åˆ†æPDFæ–‡ä»¶ï¼Œè¿”å›é¡µæ•°ã€å­—æ•°ã€è¯­è¨€ç­‰ä¿¡æ¯"""
        try:
            doc = fitz.open(input_path)
            total_pages = len(doc)

            # æå–æ‰€æœ‰æ–‡æœ¬
            all_text = ""
            for page in doc:
                text = page.get_text("text")
                all_text += text + " "

            doc.close()

            # ç»Ÿè®¡å­—æ•°
            char_count = len(all_text.strip())
            word_count = len(all_text.split())

            # æ£€æµ‹ä¸»è¦è¯­è¨€
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', all_text))
            english_chars = len(re.findall(r'[a-zA-Z]', all_text))
            japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', all_text))
            korean_chars = len(re.findall(r'[\uac00-\ud7af]', all_text))

            total_chars = len(all_text)

            # åˆ¤æ–­ä¸»è¦è¯­è¨€
            if chinese_chars > total_chars * 0.3:
                detected_lang = 'zh'
                lang_name = 'ä¸­æ–‡'
            elif english_chars > total_chars * 0.3:
                detected_lang = 'en'
                lang_name = 'è‹±è¯­'
            elif japanese_chars > total_chars * 0.2:
                detected_lang = 'ja'
                lang_name = 'æ—¥è¯­'
            elif korean_chars > total_chars * 0.2:
                detected_lang = 'ko'
                lang_name = 'éŸ©è¯­'
            else:
                detected_lang = 'auto'
                lang_name = 'æ··åˆ/å…¶ä»–'

            # ä¼°ç®—æ€»tokens
            total_tokens = self._estimate_tokens(all_text)

            return {
                'total_pages': total_pages,
                'char_count': char_count,
                'word_count': word_count,
                'detected_lang': detected_lang,
                'lang_name': lang_name,
                'total_tokens': total_tokens
            }

        except Exception as e:
            print(f'Error analyzing PDF: {e}')
            return {
                'total_pages': 0,
                'char_count': 0,
                'word_count': 0,
                'detected_lang': 'auto',
                'lang_name': 'æœªçŸ¥',
                'total_tokens': 0
            }

    def _clean_text(self, text):
        """æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®ŠUnicodeå­—ç¬¦ï¼Œé¿å…ç¼–ç é”™è¯¯"""
        if not text:
            return text

        # æ›¿æ¢å¯èƒ½æœ‰é—®é¢˜çš„Unicodeç©ºæ ¼å­—ç¬¦ä¸ºæ™®é€šç©ºæ ¼
        text = text.replace('\u00a0', ' ')  # ä¸æ¢è¡Œç©ºæ ¼
        text = text.replace('\u202f', ' ')  # çª„ä¸æ¢è¡Œç©ºæ ¼
        text = text.replace('\u2009', ' ')  # çª„ç©ºæ ¼
        text = text.replace('\u200a', ' ')  # æçª„ç©ºæ ¼
        text = text.replace('\u200b', '')   # é›¶å®½ç©ºæ ¼
        text = text.replace('\u200c', '')   # é›¶å®½éè¿æ¥ç¬¦
        text = text.replace('\u200d', '')   # é›¶å®½è¿æ¥ç¬¦
        text = text.replace('\ufeff', '')   # é›¶å®½éæ–­ç©ºæ ¼

        # æ›¿æ¢å…¶ä»–æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦ï¼‰
        import unicodedata
        text = ''.join(char for char in text
                       if unicodedata.category(char)[0] != 'C'
                       or char in '\n\r\t')

        return text

    def _estimate_tokens(self, text):
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼šä¸­æ–‡çº¦1å­—ç¬¦=1tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦=1tokenï¼‰"""
        if not text:
            return 0

        # æ£€æµ‹æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(text)

        if chinese_chars > total_chars * 0.3:
            # ä¸»è¦æ˜¯ä¸­æ–‡
            return len(text)
        else:
            # ä¸»è¦æ˜¯è‹±æ–‡æˆ–å…¶ä»–è¯­è¨€ï¼Œçº¦4å­—ç¬¦=1token
            return max(1, len(text) // 4)

    def _calculate_cost(self):
        """è®¡ç®—é¢„ä¼°è´¹ç”¨"""
        pricing = self.PRICING.get(self.api_type, {'input': 0, 'output': 0})
        input_cost = (self.input_tokens / 1_000_000) * pricing['input']
        output_cost = (self.output_tokens / 1_000_000) * pricing['output']
        return input_cost + output_cost

    def _check_cancelled(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å–æ¶ˆç¿»è¯‘"""
        if self.cancel_callback:
            self.cancel_callback()

    def _update_progress(self, current, total, message, elapsed_time=0, estimated_remaining=0):
        """æ›´æ–°è¿›åº¦"""
        # æ£€æŸ¥æ˜¯å¦å–æ¶ˆ
        self._check_cancelled()

        if self.progress_callback:
            # æ·»åŠ tokenå’Œè´¹ç”¨ä¿¡æ¯
            cost = self._calculate_cost()
            progress_data = {
                'current': current,
                'total': total,
                'percentage': int((current / total) * 100) if total > 0 else 0,
                'message': message,
                'input_tokens': self.input_tokens,
                'output_tokens': self.output_tokens,
                'estimated_cost': round(cost, 4),
                'elapsed_time': round(elapsed_time, 1),
                'estimated_remaining': round(estimated_remaining, 1)
            }
            self.progress_callback(progress_data)

    def _add_log(self, message, log_type='info'):
        """æ·»åŠ æ—¥å¿—"""
        if self.log_callback:
            self.log_callback(message, log_type)

    def _translate_text_google(self, text, source_lang='auto', target_lang='en'):
        """ä½¿ç”¨Google Translateç¿»è¯‘ï¼ˆé€šè¿‡deep-translatoråº“ï¼‰"""
        if not text or not text.strip():
            return text

        # è¯­è¨€ä»£ç æ˜ å°„ (deep-translatorä½¿ç”¨çš„ä»£ç )
        lang_mapping = {
            'zh': 'zh-CN',
            'en': 'en',
            'ja': 'ja',
            'ko': 'ko',
            'fr': 'fr',
            'de': 'de',
            'es': 'es-ES',
            'ru': 'ru',
            'ar': 'ar'
        }

        normalized_target = lang_mapping.get(target_lang, target_lang)
        # deep-translator ä½¿ç”¨ 'auto' ä½œä¸ºæºè¯­è¨€
        normalized_source = 'auto'

        # è®°å½•è¾“å…¥tokenï¼ˆç¡®ä¿æ€»æ˜¯æ‰§è¡Œï¼‰
        input_tokens = self._estimate_tokens(text)
        self.input_tokens += input_tokens

        # åªåœ¨ç¬¬ä¸€é¡µæ˜¾ç¤ºè¯¦ç»†tokenä¿¡æ¯
        if self.input_tokens == input_tokens:  # è¿™æ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨
            self._add_log(f'å¼€å§‹ç¿»è¯‘ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦, è¾“å…¥tokens: {input_tokens}', 'info')

        max_length = 4000
        if len(text) <= max_length:
            try:
                translated = GoogleTranslator(source=normalized_source, target=normalized_target).translate(text)

                # è®°å½•è¾“å‡ºtoken
                output_tokens = self._estimate_tokens(translated)
                self.output_tokens += output_tokens

                return translated
            except Exception as e:
                print(f'Translation error: {e}')
                self._add_log(f'Googleç¿»è¯‘é”™è¯¯: {str(e)}', 'error')
                return text

        # åˆ†æ®µç¿»è¯‘
        segments = []
        current_segment = ''
        sentences = text.split('. ')

        for sentence in sentences:
            if len(current_segment) + len(sentence) < max_length:
                current_segment += sentence + '. '
            else:
                if current_segment:
                    segments.append(current_segment.strip())
                current_segment = sentence + '. '

        if current_segment:
            segments.append(current_segment.strip())

        translated_segments = []
        for i, segment in enumerate(segments):
            try:
                self._add_log(f'ç¿»è¯‘æ®µè½ {i+1}/{len(segments)}', 'info')
                translated = GoogleTranslator(source=normalized_source, target=normalized_target).translate(segment)
                translated_segments.append(translated)

                # è®°å½•è¾“å‡ºtoken
                output_tokens = self._estimate_tokens(translated)
                self.output_tokens += output_tokens

            except Exception as e:
                print(f'Translation error in segment: {e}')
                self._add_log(f'æ®µè½ç¿»è¯‘é”™è¯¯: {str(e)}', 'error')
                translated_segments.append(segment)

        return '. '.join(translated_segments)

    def _translate_text_deepseek(self, text, source_lang='auto', target_lang='en'):
        """ä½¿ç”¨DeepSeek APIç¿»è¯‘"""
        if not text or not text.strip():
            return text

        try:
            # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®ŠUnicodeå­—ç¬¦
            text = self._clean_text(text)

            url = "https://api.deepseek.com/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json; charset=utf-8"
            }

            lang_names = {
                'en': 'è‹±è¯­',
                'zh': 'ä¸­æ–‡',
                'ja': 'æ—¥è¯­',
                'ko': 'éŸ©è¯­',
                'fr': 'æ³•è¯­',
                'de': 'å¾·è¯­',
                'es': 'è¥¿ç­ç‰™è¯­',
                'ru': 'ä¿„è¯­',
                'ar': 'é˜¿æ‹‰ä¼¯è¯­'
            }

            target_lang_name = lang_names.get(target_lang, target_lang)
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang_name}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\n\n{text}"

            # è®°å½•è¾“å…¥token
            input_tokens = self._estimate_tokens(text)
            self.input_tokens += input_tokens

            # åªåœ¨ç¬¬ä¸€æ¬¡æ˜¾ç¤ºè¯¦ç»†tokenä¿¡æ¯
            if self.input_tokens == input_tokens:
                self._add_log(f'å¼€å§‹ç¿»è¯‘ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦, è¾“å…¥tokens: {input_tokens}', 'info')

            data = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3
            }

            response = requests.post(url, headers=headers, json=data, timeout=60)
            result = response.json()

            if 'choices' in result and len(result['choices']) > 0:
                translated = result['choices'][0]['message']['content'].strip()

                # è®°å½•è¾“å‡ºtoken
                output_tokens = self._estimate_tokens(translated)
                self.output_tokens += output_tokens

                return translated
            else:
                raise Exception(f"API Error: {result}")

        except Exception as e:
            error_msg = str(e)
            print(f'DeepSeek translation error: {error_msg}')

            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
            if '401' in error_msg or 'auth' in error_msg.lower() or 'Invalid' in error_msg:
                self._add_log('âŒ DeepSeek API Keyæ— æ•ˆï¼è¯·æ£€æŸ¥API Keyè®¾ç½®', 'error')
                self._add_log('ğŸ’¡ å»ºè®®ï¼šè¯·ä½¿ç”¨"Googleç¿»è¯‘ï¼ˆå…è´¹ï¼‰"é€‰é¡¹ï¼Œæ— éœ€API Key', 'info')
                self._add_log('ğŸ’¡ æˆ–è€…è·å–DeepSeek API Key: https://platform.deepseek.com/', 'info')
            else:
                self._add_log(f'DeepSeekç¿»è¯‘é”™è¯¯: {error_msg}', 'error')

            return text

    def _translate_text_zhipu(self, text, source_lang='auto', target_lang='en'):
        """ä½¿ç”¨æ™ºè°±AI APIç¿»è¯‘"""
        if not text or not text.strip():
            return text

        try:
            # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®ŠUnicodeå­—ç¬¦
            text = self._clean_text(text)

            url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json; charset=utf-8"
            }

            lang_names = {
                'en': 'è‹±è¯­',
                'zh': 'ä¸­æ–‡',
                'ja': 'æ—¥è¯­',
                'ko': 'éŸ©è¯­',
                'fr': 'æ³•è¯­',
                'de': 'å¾·è¯­',
                'es': 'è¥¿ç­ç‰™è¯­',
                'ru': 'ä¿„è¯­',
                'ar': 'é˜¿æ‹‰ä¼¯è¯­'
            }

            target_lang_name = lang_names.get(target_lang, target_lang)
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang_name}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\n\n{text}"

            # è®°å½•è¾“å…¥token
            input_tokens = self._estimate_tokens(text)
            self.input_tokens += input_tokens

            data = {
                "model": "GLM-4-Flash",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3
            }

            response = requests.post(url, headers=headers, json=data, timeout=60)
            result = response.json()

            if 'choices' in result and len(result['choices']) > 0:
                translated = result['choices'][0]['message']['content'].strip()

                # è®°å½•è¾“å‡ºtoken
                output_tokens = self._estimate_tokens(translated)
                self.output_tokens += output_tokens

                return translated
            else:
                raise Exception(f"API Error: {result}")

        except Exception as e:
            error_msg = str(e)
            print(f'Zhipu AI translation error: {error_msg}')

            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
            if '401' in error_msg or 'auth' in error_msg.lower() or 'Invalid' in error_msg:
                self._add_log('âŒ æ™ºè°±AI API Keyæ— æ•ˆï¼è¯·æ£€æŸ¥API Keyè®¾ç½®', 'error')
                self._add_log('ğŸ’¡ å»ºè®®ï¼šè¯·ä½¿ç”¨"Googleç¿»è¯‘ï¼ˆå…è´¹ï¼‰"é€‰é¡¹ï¼Œæ— éœ€API Key', 'info')
                self._add_log('ğŸ’¡ æˆ–è€…è·å–æ™ºè°±AI API Key: https://open.bigmodel.cn/', 'info')
            else:
                self._add_log(f'æ™ºè°±AIç¿»è¯‘é”™è¯¯: {error_msg}', 'error')

            return text

    def _translate_text_openrouter(self, text, source_lang='auto', target_lang='en'):
        """ä½¿ç”¨OpenRouterçš„DeepSeek APIç¿»è¯‘"""
        if not text or not text.strip():
            return text

        try:
            # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®ŠUnicodeå­—ç¬¦
            text = self._clean_text(text)

            url = "https://openrouter.ai/api/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json; charset=utf-8",
                "HTTP-Referer": "https://pdf-translator.local",  # OpenRouterè¦æ±‚
            }

            lang_names = {
                'en': 'è‹±è¯­',
                'zh': 'ä¸­æ–‡',
                'ja': 'æ—¥è¯­',
                'ko': 'éŸ©è¯­',
                'fr': 'æ³•è¯­',
                'de': 'å¾·è¯­',
                'es': 'è¥¿ç­ç‰™è¯­',
                'ru': 'ä¿„è¯­',
                'ar': 'é˜¿æ‹‰ä¼¯è¯­'
            }

            target_lang_name = lang_names.get(target_lang, target_lang)
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang_name}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\n\n{text}"

            # è®°å½•è¾“å…¥token
            input_tokens = self._estimate_tokens(text)
            self.input_tokens += input_tokens

            # åªåœ¨ç¬¬ä¸€æ¬¡æ˜¾ç¤ºè¯¦ç»†tokenä¿¡æ¯
            if self.input_tokens == input_tokens:
                self._add_log(f'å¼€å§‹ç¿»è¯‘ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦, è¾“å…¥tokens: {input_tokens}', 'info')

            data = {
                "model": "deepseek/deepseek-chat",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3
            }

            response = requests.post(url, headers=headers, json=data, timeout=60)
            result = response.json()

            if 'choices' in result and len(result['choices']) > 0:
                translated = result['choices'][0]['message']['content'].strip()

                # è®°å½•è¾“å‡ºtoken
                output_tokens = self._estimate_tokens(translated)
                self.output_tokens += output_tokens

                return translated
            else:
                raise Exception(f"API Error: {result}")

        except Exception as e:
            error_msg = str(e)
            print(f'OpenRouter translation error: {error_msg}')

            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
            if '401' in error_msg or 'auth' in error_msg.lower() or 'cookie' in error_msg.lower():
                self._add_log('âŒ OpenRouter API Keyæ— æ•ˆæˆ–æœªæ­£ç¡®è®¾ç½®ï¼', 'error')
                self._add_log('ğŸ’¡ å»ºè®®ï¼šè¯·ä½¿ç”¨"Googleç¿»è¯‘ï¼ˆå…è´¹ï¼‰"é€‰é¡¹ï¼Œæ— éœ€API Key', 'info')
                self._add_log('ğŸ’¡ æˆ–è€…åœ¨OpenRouterè·å–æœ‰æ•ˆçš„API Key: https://openrouter.ai/keys', 'info')
            else:
                self._add_log(f'OpenRouterç¿»è¯‘é”™è¯¯: {error_msg}', 'error')

            return text

    def _translate_text_kimi(self, text, source_lang='auto', target_lang='en'):
        """ä½¿ç”¨OpenRouterçš„Kimi (moonshot-v1-auto) APIç¿»è¯‘"""
        if not text or not text.strip():
            return text

        try:
            # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®ŠUnicodeå­—ç¬¦
            text = self._clean_text(text)

            url = "https://openrouter.ai/api/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json; charset=utf-8",
                "HTTP-Referer": "https://pdf-translator.local",
            }

            lang_names = {
                'en': 'è‹±è¯­',
                'zh': 'ä¸­æ–‡',
                'ja': 'æ—¥è¯­',
                'ko': 'éŸ©è¯­',
                'fr': 'æ³•è¯­',
                'de': 'å¾·è¯­',
                'es': 'è¥¿ç­ç‰™è¯­',
                'ru': 'ä¿„è¯­',
                'ar': 'é˜¿æ‹‰ä¼¯è¯­'
            }

            target_lang_name = lang_names.get(target_lang, target_lang)
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang_name}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šï¼š\n\n{text}"

            # è®°å½•è¾“å…¥token
            input_tokens = self._estimate_tokens(text)
            self.input_tokens += input_tokens

            # åªåœ¨ç¬¬ä¸€æ¬¡æ˜¾ç¤ºè¯¦ç»†tokenä¿¡æ¯
            if self.input_tokens == input_tokens:
                self._add_log(f'å¼€å§‹ç¿»è¯‘ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦, è¾“å…¥tokens: {input_tokens}', 'info')

            data = {
                "model": "moonshot/moonshot-v1-auto",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å‡†ç¡®ç¿»è¯‘æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡çš„æ ¼å¼å’Œè¯­æ°”ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1
            }

            response = requests.post(url, headers=headers, json=data, timeout=120)
            result = response.json()

            if 'choices' in result and len(result['choices']) > 0:
                translated = result['choices'][0]['message']['content'].strip()

                # è®°å½•è¾“å‡ºtoken
                output_tokens = self._estimate_tokens(translated)
                self.output_tokens += output_tokens

                return translated
            else:
                raise Exception(f"API Error: {result}")

        except Exception as e:
            error_msg = str(e)
            print(f'Kimi translation error: {error_msg}')

            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
            if '401' in error_msg or 'auth' in error_msg.lower() or 'cookie' in error_msg.lower():
                self._add_log('âŒ Kimi API Keyæ— æ•ˆæˆ–æœªæ­£ç¡®è®¾ç½®ï¼', 'error')
                self._add_log('ğŸ’¡ å»ºè®®ï¼šè¯·ä½¿ç”¨"Googleç¿»è¯‘ï¼ˆå…è´¹ï¼‰"é€‰é¡¹ï¼Œæ— éœ€API Key', 'info')
            else:
                self._add_log(f'Kimiç¿»è¯‘é”™è¯¯: {error_msg}', 'error')

            return text

    def _translate_text_gpt(self, text, source_lang='auto', target_lang='en'):
        """ä½¿ç”¨OpenRouterçš„GPT-4.1 APIç¿»è¯‘"""
        if not text or not text.strip():
            return text

        try:
            # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®ŠUnicodeå­—ç¬¦
            text = self._clean_text(text)

            url = "https://openrouter.ai/api/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json; charset=utf-8",
                "HTTP-Referer": "https://pdf-translator.local",
            }

            lang_names = {
                'en': 'è‹±è¯­',
                'zh': 'ä¸­æ–‡',
                'ja': 'æ—¥è¯­',
                'ko': 'éŸ©è¯­',
                'fr': 'æ³•è¯­',
                'de': 'å¾·è¯­',
                'es': 'è¥¿ç­ç‰™è¯­',
                'ru': 'ä¿„è¯­',
                'ar': 'é˜¿æ‹‰ä¼¯è¯­'
            }

            target_lang_name = lang_names.get(target_lang, target_lang)
            prompt = f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang_name}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šï¼š\n\n{text}"

            # è®°å½•è¾“å…¥token
            input_tokens = self._estimate_tokens(text)
            self.input_tokens += input_tokens

            # åªåœ¨ç¬¬ä¸€æ¬¡æ˜¾ç¤ºè¯¦ç»†tokenä¿¡æ¯
            if self.input_tokens == input_tokens:
                self._add_log(f'å¼€å§‹ç¿»è¯‘ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦, è¾“å…¥tokens: {input_tokens}', 'info')

            data = {
                "model": "openai/gpt-4-turbo",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å‡†ç¡®ç¿»è¯‘æ–‡æœ¬ï¼Œä¿æŒåŸæ–‡çš„æ ¼å¼å’Œè¯­æ°”ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1
            }

            response = requests.post(url, headers=headers, json=data, timeout=120)
            result = response.json()

            if 'choices' in result and len(result['choices']) > 0:
                translated = result['choices'][0]['message']['content'].strip()

                # è®°å½•è¾“å‡ºtoken
                output_tokens = self._estimate_tokens(translated)
                self.output_tokens += output_tokens

                return translated
            else:
                raise Exception(f"API Error: {result}")

        except Exception as e:
            error_msg = str(e)
            print(f'GPT translation error: {error_msg}')

            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¤è¯é”™è¯¯
            if '401' in error_msg or 'auth' in error_msg.lower() or 'cookie' in error_msg.lower():
                self._add_log('âŒ GPT API Keyæ— æ•ˆæˆ–æœªæ­£ç¡®è®¾ç½®ï¼', 'error')
                self._add_log('ğŸ’¡ å»ºè®®ï¼šè¯·ä½¿ç”¨"Googleç¿»è¯‘ï¼ˆå…è´¹ï¼‰"é€‰é¡¹ï¼Œæ— éœ€API Key', 'info')
            else:
                self._add_log(f'GPTç¿»è¯‘é”™è¯¯: {error_msg}', 'error')

            return text

    def _translate_text(self, text, source_lang='auto', target_lang='en'):
        """æ ¹æ®APIç±»å‹é€‰æ‹©ç¿»è¯‘æ–¹æ³•"""
        if self.api_type == 'google':
            return self._translate_text_google(text, source_lang, target_lang)
        elif self.api_type == 'deepseek':
            return self._translate_text_deepseek(text, source_lang, target_lang)
        elif self.api_type == 'zhipu':
            return self._translate_text_zhipu(text, source_lang, target_lang)
        elif self.api_type == 'openrouter':
            return self._translate_text_openrouter(text, source_lang, target_lang)
        elif self.api_type == 'kimi':
            return self._translate_text_kimi(text, source_lang, target_lang)
        elif self.api_type == 'gpt':
            return self._translate_text_gpt(text, source_lang, target_lang)
        else:
            return text

    def _translate_text_batch(self, texts, source_lang='auto', target_lang='en'):
        """æ‰¹é‡ç¿»è¯‘å¤šä¸ªæ–‡æœ¬ï¼Œæé«˜é€Ÿåº¦"""
        if not texts or len(texts) == 0:
            return []

        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [(i, text) for i, text in enumerate(texts) if text and text.strip()]
        if not valid_texts:
            return texts

        # å¯¹äºGoogle Translateï¼Œä½¿ç”¨æ‰¹é‡ç¿»è¯‘
        if self.api_type == 'google':
            return self._translate_text_batch_google(valid_texts, source_lang, target_lang)
        else:
            # å¯¹äºå…¶ä»–APIï¼Œé€ä¸ªç¿»è¯‘ï¼ˆä½†å‡å°‘æ—¥å¿—ï¼‰
            results = [None] * len(texts)
            for idx, text in valid_texts:
                results[idx] = self._translate_text(text, source_lang, target_lang)
            return results

    def _translate_text_batch_google(self, valid_texts, source_lang='auto', target_lang='en'):
        """Google Translateæ‰¹é‡ç¿»è¯‘ - ä½¿ç”¨é«˜å¹¶å‘è¯·æ±‚ï¼ˆé€šè¿‡deep-translatorï¼‰"""
        import concurrent.futures
        import threading

        try:
            # è¯­è¨€ä»£ç æ˜ å°„
            lang_mapping = {
                'zh': 'zh-CN',
                'en': 'en',
                'ja': 'ja',
                'ko': 'ko',
                'fr': 'fr',
                'de': 'de',
                'es': 'es-ES',
                'ru': 'ru',
                'ar': 'ar'
            }

            normalized_target = lang_mapping.get(target_lang, target_lang)
            normalized_source = 'auto'  # deep-translator ä½¿ç”¨ auto

            results = {}
            lock = threading.Lock()

            # å¢åŠ å¹¶å‘æ•°åˆ°10ï¼Œæé«˜é€Ÿåº¦
            max_workers = 10

            def translate_single(idx, text):
                try:
                    # æ¸…ç†æ–‡æœ¬
                    text = self._clean_text(text)

                    # è®°å½•è¾“å…¥token
                    input_tokens = self._estimate_tokens(text)
                    with lock:
                        self.input_tokens += input_tokens

                    # ç¿»è¯‘
                    translated = GoogleTranslator(source=normalized_source, target=normalized_target).translate(text)

                    # è®°å½•è¾“å‡ºtoken
                    output_tokens = self._estimate_tokens(translated)
                    with lock:
                        self.output_tokens += output_tokens

                    return (idx, translated, None)
                except Exception as e:
                    print(f'Translation error for text {idx}: {e}')
                    return (idx, text, str(e))

            # å¹¶å‘ç¿»è¯‘æ‰€æœ‰æ–‡æœ¬
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(translate_single, idx, text): idx for idx, text in valid_texts}

                for future in concurrent.futures.as_completed(futures):
                    try:
                        idx, translated, error = future.result()
                        with lock:
                            results[idx] = translated
                    except Exception as e:
                        idx = futures[future]
                        print(f'Future {idx} failed: {e}')
                        with lock:
                            results[idx] = valid_texts[idx][1]

            # è¿”å›ç»“æœæ•°ç»„ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼ŒåŒ…æ‹¬Noneå€¼ï¼‰
            return [results.get(i, None) for i in range(len(valid_texts))]

        except Exception as e:
            print(f'Concurrent translation failed: {e}')
            import traceback
            traceback.print_exc()
            # é™çº§åˆ°é€ä¸ªç¿»è¯‘
            results = {}
            for idx, text in valid_texts:
                try:
                    text = self._clean_text(text)
                    translated = GoogleTranslator(source=normalized_source, target=normalized_target).translate(text)
                    results[idx] = translated
                except Exception as e:
                    print(f'Single translation error: {e}')
                    results[idx] = text
            return [results.get(i, None) for i in range(len(valid_texts))]

    def translate_pdf(self, input_path, output_path, source_lang='auto', target_lang='en', concurrency=4):
        """ç¿»è¯‘PDFæ–‡ä»¶ï¼ˆå¹¶å‘ç¿»è¯‘ï¼‰"""
        import concurrent.futures
        import threading
        import os

        # è°ƒè¯•ï¼šæ‰“å°å¹¶å‘å‚æ•°
        print(f"[DEBUG] translate_pdf called with concurrency={concurrency}")

        doc = None
        try:
            self._add_log('========== å¼€å§‹ç¿»è¯‘ä»»åŠ¡ ==========', 'info')
            self._add_log(f'ä½¿ç”¨æ–‡æœ¬å—çº§å¹¶å‘ç¿»è¯‘ï¼Œ{concurrency} ä¸ªçº¿ç¨‹åŒæ—¶å·¥ä½œ âš¡', 'success')

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            self._add_log(f'è¾“å…¥æ–‡ä»¶: {input_path}', 'info')
            self._add_log(f'è¾“å‡ºæ–‡ä»¶: {output_path}', 'info')

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            self._add_log('æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å¤§å°...', 'info')
            file_size = os.path.getsize(input_path)
            file_size_mb = file_size / (1024 * 1024)
            self._add_log(f'æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB', 'info')

            # æ‰“å¼€åŸå§‹PDF
            self._add_log('æ­£åœ¨æ‰“å¼€PDFæ–‡ä»¶...', 'info')
            start_time = time.time()
            translation_start_time = start_time

            try:
                doc = fitz.open(input_path)
                open_time = time.time() - start_time
                self._add_log(f'âœ“ PDFæ‰“å¼€æˆåŠŸ (è€—æ—¶ {open_time:.2f}ç§’)', 'success')
            except Exception as e:
                self._add_log(f'âœ— PDFæ‰“å¼€å¤±è´¥: {str(e)}', 'error')
                raise

            total_pages = len(doc)
            self._add_log(f'PDFæ€»é¡µæ•°: {total_pages} é¡µ', 'info')

            # æ¯Né¡µè®°å½•ä¸€æ¬¡æ—¥å¿—
            log_interval = max(1, total_pages // 10)

            # è¯­è¨€ä»£ç æ˜ å°„
            lang_mapping = {
                'zh': 'zh-CN',
                'en': 'en',
                'ja': 'ja',
                'ko': 'ko',
                'fr': 'fr',
                'de': 'de',
                'es': 'es',
                'ru': 'ru',
                'ar': 'ar'
            }

            normalized_target = lang_mapping.get(target_lang, target_lang)
            normalized_source = 'auto' if source_lang == 'auto' else lang_mapping.get(source_lang, source_lang)

            # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å¹¶å‘æ•°
            self._add_log(f'ğŸ“– å¼€å§‹æå–å’Œç¿»è¯‘ï¼ˆå¹¶å‘æ•°: {concurrency}ï¼‰...', 'info')
            self._add_log(f'âš¡ ä½¿ç”¨ {concurrency} ä¸ªçº¿ç¨‹å¹¶å‘ç¿»è¯‘', 'success')

            # å­˜å‚¨æ¯é¡µçš„ç¿»è¯‘ç»“æœ: {page_num: [(rect, translated_text), ...]}
            page_translations_map = {}

            # æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬å—
            all_blocks = []
            for page_num in range(total_pages):
                page = doc[page_num]
                blocks = page.get_text("blocks")
                blocks.sort(key=lambda b: (b[1], b[0]))

                for block_idx, block in enumerate(blocks):
                    if block[6] == 0:  # æ–‡æœ¬å—
                        text = block[4]
                        if text and text.strip():
                            all_blocks.append({
                                'page_num': page_num,
                                'block_idx': block_idx,
                                'text': text,
                                'rect': fitz.Rect(block[0], block[1], block[2], block[3])
                            })

            total_blocks = len(all_blocks)
            self._add_log(f'æ€»å…±æå–åˆ° {total_blocks} ä¸ªæ–‡æœ¬å—', 'info')

            # æ˜¾ç¤ºæ¯ä¸ªæ–‡æœ¬å—çš„åŸæ–‡ï¼ˆæŒ‰é¡µåˆ†ç»„ï¼‰
            # åªæ˜¾ç¤ºå‰3é¡µå’Œå3é¡µçš„åŸæ–‡ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
            self._add_log('=' * 60, 'info')
            self._add_log('åŸæ–‡æå–ï¼ˆå‰3é¡µå’Œå3é¡µï¼‰ï¼š', 'info')
            current_page = -1
            for block_info in all_blocks:
                page_num = block_info['page_num']
                # åªæ˜¾ç¤ºå‰3é¡µå’Œå3é¡µ
                if page_num >= 3 and page_num < total_pages - 3:
                    if current_page != page_num:
                        current_page = page_num
                        self._add_log(f'--- ç¬¬ {page_num + 1} é¡µï¼ˆå·²è·³è¿‡ï¼‰ ---', 'info')
                    continue

                if block_info['page_num'] != current_page:
                    current_page = block_info['page_num']
                    self._add_log(f'--- ç¬¬ {current_page + 1} é¡µ ---', 'info')

                text = block_info['text']
                display_text = text[:200] + '...' if len(text) > 200 else text
                # ä½¿ç”¨å‰ç«¯æœŸæœ›çš„æ ¼å¼ï¼š[æ–‡æœ¬å— X] åŸæ–‡: ... (å¸¦é¡µç )
                self._add_log(f'[é¡µ{block_info["page_num"] + 1}|æ–‡æœ¬å— {block_info["block_idx"] + 1}] åŸæ–‡: {display_text}', 'info')

            # å¹¶å‘ç¿»è¯‘æ‰€æœ‰æ–‡æœ¬å—
            self._add_log('=' * 60, 'info')
            self._add_log('å¼€å§‹ç¿»è¯‘...', 'info')

            results = {}
            completed_count = [0]
            lock = threading.Lock()

            def translate_block(block_info):
                """ç¿»è¯‘å•ä¸ªæ–‡æœ¬å—"""
                max_retries = 3
                translated = None
                api_start_time = time.time()

                for attempt in range(max_retries):
                    try:
                        self._check_cancelled()

                        text = self._clean_text(block_info['text'])

                        # åœ¨ç¿»è¯‘å‰å‘é€åŸæ–‡æ—¥å¿—ï¼ˆå‰ç«¯æœŸæœ›æ ¼å¼: [åŸæ–‡ åºå·/æ€»æ•°] å†…å®¹ï¼‰
                        with lock:
                            current = completed_count[0] + 1
                            display_original = text[:200] + '...' if len(text) > 200 else text
                            self._add_log(f'[åŸæ–‡ {current}/{total_blocks}] {display_original}', 'info')

                        # è®°å½•è¾“å…¥token
                        input_tokens = self._estimate_tokens(text)

                        # ç¿»è¯‘
                        if self.api_type == 'google':
                            translated = GoogleTranslator(source=normalized_source, target=normalized_target).translate(text)
                        else:
                            translated = self._translate_text(text, source_lang, target_lang)

                        # è®°å½•è¾“å‡ºtoken
                        output_tokens = self._estimate_tokens(translated)
                        api_time = time.time() - api_start_time

                        with lock:
                            # å‘é€è¯‘æ–‡æ—¥å¿—ï¼ˆå‰ç«¯æœŸæœ›æ ¼å¼: [è¯‘æ–‡ åºå·/æ€»æ•°] å†…å®¹ (è€—æ—¶: Xs)ï¼‰
                            display_translated = translated[:200] + '...' if len(translated) > 200 else translated
                            self._add_log(f'[è¯‘æ–‡ {current}/{total_blocks}] {display_translated} (è€—æ—¶: {api_time:.1f}s)', 'success')

                            self.input_tokens += input_tokens
                            self.output_tokens += output_tokens
                            completed_count[0] += 1

                            # æ›´æ–°è¿›åº¦
                            elapsed_time = time.time() - translation_start_time
                            current = completed_count[0]
                            self._update_progress(
                                current,
                                total_blocks,
                                f'å·²ç¿»è¯‘ {current}/{total_blocks} ä¸ªæ–‡æœ¬å—...',
                                elapsed_time=elapsed_time,
                                estimated_remaining=0
                            )

                        return block_info, translated

                    except Exception as e:
                        print(f'Translation error for block {block_info["block_idx"]} (attempt {attempt + 1}): {e}')

                        if attempt == max_retries - 1:
                            error_msg = str(e)
                            with lock:
                                completed_count[0] += 1
                                elapsed_time = time.time() - translation_start_time
                                self._update_progress(
                                    completed_count[0],
                                    total_blocks,
                                    f'å·²ç¿»è¯‘ {completed_count[0]}/{total_blocks} ä¸ªæ–‡æœ¬å—...',
                                    elapsed_time=elapsed_time,
                                    estimated_remaining=0
                                )
                            return block_info, block_info['text']
                        else:
                            time.sleep(0.5)

            # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å¹¶å‘æ•°ç¿»è¯‘
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = {executor.submit(translate_block, block): block for block in all_blocks}

                for future in concurrent.futures.as_completed(futures):
                    try:
                        block_info, translated_text = future.result()
                        results[(block_info['page_num'], block_info['block_idx'])] = (block_info['rect'], translated_text)

                        # æ¯å®Œæˆä¸€å®šæ•°é‡æ˜¾ç¤ºæ—¥å¿—
                        if completed_count[0] % 20 == 0 or completed_count[0] >= total_blocks - 10:
                            self._add_log(f'è¿›åº¦: {completed_count[0]}/{total_blocks} æ–‡æœ¬å—å·²ç¿»è¯‘', 'info')

                    except Exception as e:
                        print(f'Future error: {e}')

            self._add_log(f'âœ“ æ‰€æœ‰æ–‡æœ¬å—ç¿»è¯‘å®Œæˆ', 'success')

            # è°ƒè¯•ï¼šæ£€æŸ¥resultså­—å…¸
            self._add_log(f'[DEBUG] resultså­—å…¸åŒ…å« {len(results)} ä¸ªç¿»è¯‘ç»“æœ', 'info')
            # æ˜¾ç¤ºå‰å‡ ä¸ªresultsçš„key
            for i, (key, value) in enumerate(list(results.items())[:3]):
                page_num, block_idx = key
                rect, text = value
                self._add_log(f'[DEBUG] results[({page_num},{block_idx})]: rect={rect}, æ–‡æœ¬é•¿åº¦={len(text)}', 'info')

            # æŒ‰é¡µç»„ç»‡ç¿»è¯‘ç»“æœå¹¶æ˜¾ç¤º
            # åªæ˜¾ç¤ºå‰3é¡µå’Œå3é¡µçš„è¯‘æ–‡ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
            self._add_log('=' * 60, 'info')
            self._add_log('ç¿»è¯‘ç»“æœï¼ˆå‰3é¡µå’Œå3é¡µï¼‰ï¼š', 'info')

            for page_num in range(total_pages):
                page_translations = []

                # è·å–è¿™ä¸€é¡µçš„æ‰€æœ‰æ–‡æœ¬å—
                page_blocks = [b for b in all_blocks if b['page_num'] == page_num]

                if not page_blocks:
                    continue

                # è·³è¿‡ä¸­é—´é¡µé¢çš„è¯¦ç»†æ˜¾ç¤º
                if page_num < 3 or page_num >= total_pages - 3:
                    self._add_log(f'--- ç¬¬ {page_num + 1} é¡µ ---', 'info')

                for block_info in page_blocks:
                    block_idx = block_info['block_idx']
                    result = results.get((page_num, block_idx))
                    if result:
                        rect, translated_text = result
                        page_translations.append((rect, translated_text))

                        # åªæ˜¾ç¤ºå‰3é¡µå’Œå3é¡µçš„è¯‘æ–‡
                        if page_num < 3 or page_num >= total_pages - 3:
                            display_translated = translated_text[:200] + '...' if len(translated_text) > 200 else translated_text
                            # ä½¿ç”¨å‰ç«¯æœŸæœ›çš„æ ¼å¼ï¼š[é¡µX|æ–‡æœ¬å— Y] è¯‘æ–‡: ...
                            self._add_log(f'[é¡µ{page_num + 1}|æ–‡æœ¬å— {block_idx + 1}] è¯‘æ–‡: {display_translated}', 'success')
                    else:
                        self._add_log(f'âš ï¸ ç¬¬{page_num + 1}é¡µå—{block_idx + 1}ç¿»è¯‘ç»“æœä¸¢å¤±', 'error')

                page_translations_map[page_num] = page_translations

                # è®°å½•è·³è¿‡çš„é¡µé¢
                if page_num >= 3 and page_num < total_pages - 3:
                    self._add_log(f'--- ç¬¬ {page_num + 1} é¡µï¼ˆå·²è·³è¿‡ï¼Œ{len(page_translations)} ä¸ªæ–‡æœ¬å—ï¼‰ ---', 'info')

            self._add_log(f'âœ“ æ‰€æœ‰é¡µé¢ç¿»è¯‘å®Œæˆ', 'success')

            # å°†ç¿»è¯‘ç»“æœå†™å›PDF
            self._add_log('æ­£åœ¨å°†è¯‘æ–‡å†™å›PDF...', 'info')
            self._add_log('å°†å½»åº•ç§»é™¤åŸæ–‡å¹¶æ’å…¥ç¿»è¯‘ï¼Œä¿ç•™å›¾ç‰‡å’Œæ’ç‰ˆæ ¼å¼', 'info')

            # è°ƒè¯•ï¼šæ£€æŸ¥page_translations_map
            self._add_log(f'[DEBUG] page_translations_mapåŒ…å« {len(page_translations_map)} é¡µ', 'info')
            for page_num, translations in list(page_translations_map.items())[:3]:  # åªæ˜¾ç¤ºå‰3é¡µ
                self._add_log(f'[DEBUG] ç¬¬{page_num + 1}é¡µæœ‰ {len(translations)} ä¸ªç¿»è¯‘', 'info')

            total_written = 0

            # æ–¹æ³•ï¼šåˆ›å»ºæ–°æ–‡æ¡£ï¼Œå¤åˆ¶åŸé¡µé¢çš„å›¾ç‰‡å’Œå›¾å½¢ï¼Œç„¶ååªæ·»åŠ ç¿»è¯‘åçš„æ–‡æœ¬
            # è¿™æ ·å¯ä»¥å½»åº•ç§»é™¤åŸæ–‡ï¼ŒåŒæ—¶ä¿ç•™å›¾ç‰‡å’Œæ’ç‰ˆ

            self._add_log('æ­£åœ¨åˆ›å»ºæ–°æ–‡æ¡£ï¼ˆä¿ç•™å›¾ç‰‡ï¼Œç§»é™¤åŸæ–‡ï¼‰...', 'info')
            new_doc = fitz.open()

            for page_num in range(total_pages):
                self._check_cancelled()

                page = doc[page_num]
                page_translations = page_translations_map.get(page_num, [])

                # è·å–åŸé¡µé¢çš„å°ºå¯¸å’Œæ—‹è½¬
                mediabox = page.mediabox
                rotation = page.rotation

                # åˆ›å»ºæ–°é¡µé¢
                new_page = new_doc.new_page(
                    width=mediabox.width,
                    height=mediabox.height
                )

                # è®¾ç½®é¡µé¢æ—‹è½¬
                if rotation:
                    new_page.set_rotation(rotation)

                # å¡«å……ç™½è‰²èƒŒæ™¯
                new_page.draw_rect(new_page.rect, color=(1, 1, 1), fill=(1, 1, 1))

                # å¤åˆ¶åŸé¡µé¢çš„æ‰€æœ‰å›¾ç‰‡
                try:
                    image_list = page.get_images()
                    self._add_log(f'[DEBUG] ç¬¬{page_num+1}é¡µæœ‰ {len(image_list)} å¼ å›¾ç‰‡', 'info')

                    for img_index, img in enumerate(image_list):
                        try:
                            xref = img[0]
                            # è·å–å›¾ç‰‡åœ¨é¡µé¢ä¸Šçš„ä½ç½®
                            img_rects = page.get_image_rects(xref)
                            for img_rect in img_rects:
                                # åœ¨æ–°é¡µé¢ä¸Šç»˜åˆ¶å›¾ç‰‡
                                new_page.insert_image(img_rect, pixmap=fitz.Pixmap(doc, xref))
                        except Exception as img_err:
                            self._add_log(f'[DEBUG] å›¾ç‰‡å¤åˆ¶å¤±è´¥: {str(img_err)[:50]}', 'info')
                except Exception as e:
                    self._add_log(f'[DEBUG] å›¾ç‰‡å¤„ç†å‡ºé”™: {str(e)[:50]}', 'info')

                # å¤åˆ¶åŸé¡µé¢çš„å›¾å½¢ï¼ˆçº¿æ¡ã€å½¢çŠ¶ç­‰ï¼‰
                try:
                    # è·å–é¡µé¢çš„ç»˜å›¾å†…å®¹
                    # ä½¿ç”¨ get_text("rawdict") æˆ–å…¶ä»–æ–¹æ³•è·å–å›¾å½¢ä¿¡æ¯
                    # è¿™é‡Œæˆ‘ä»¬ç®€å•ä½¿ç”¨ page.get_svg_image() æ¥è·å–æ‰€æœ‰è§†è§‰å…ƒç´ 
                    pass
                except Exception as e:
                    self._add_log(f'[DEBUG] å›¾å½¢å¤„ç†å‡ºé”™: {str(e)[:50]}', 'info')

                if not page_translations:
                    if page_num < 3 or page_num >= total_pages - 3:
                        self._add_log(f'ç¬¬ {page_num + 1} é¡µ: æ— ç¿»è¯‘å†…å®¹', 'info')
                    continue

                if page_num < 3 or page_num >= total_pages - 3:
                    self._add_log(f'æ›´æ–°ç¬¬ {page_num + 1} é¡µï¼ˆ{len(page_translations)} ä¸ªæ–‡æœ¬å—ï¼‰...', 'info')
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æœ¬å—çš„ä¿¡æ¯
                    if page_translations:
                        first_rect, first_text = page_translations[0]
                        self._add_log(f'[DEBUG] ç¬¬ä¸€ä¸ªæ–‡æœ¬å—: rect={first_rect}, æ–‡æœ¬é•¿åº¦={len(first_text)}', 'info')
                        self._add_log(f'[DEBUG] æ–‡æœ¬é¢„è§ˆ: {first_text[:100]}', 'info')

                # æ›´æ–°è¿™ä¸€é¡µçš„å†…å®¹
                success_count = 0
                for idx, (text_rect, translated_text) in enumerate(page_translations):
                    try:
                        self._add_log(f'[DEBUG] å¼€å§‹å†™å…¥ç¬¬{page_num+1}é¡µå—{idx+1}: rect=({text_rect.x0:.1f},{text_rect.y0:.1f},{text_rect.x1:.1f},{text_rect.y1:.1f}), æ–‡æœ¬é•¿åº¦={len(translated_text)}', 'info')

                        # å†™å…¥ç¿»è¯‘æ–‡æœ¬
                        try:
                            # ä½¿ç”¨ fitz çš„å†…ç½®ä¸­æ–‡æ”¯æŒ
                            result = new_page.insert_textbox(
                                text_rect,
                                translated_text,
                                fontsize=11,
                                fontname="china-s",  # ä½¿ç”¨ç®€ä½“ä¸­æ–‡å­—ä½“
                                color=(0, 0, 0),
                                align=0
                            )

                            if result >= 0:
                                success_count += 1
                                total_written += 1
                                self._add_log(f'[DEBUG] æ–‡æœ¬å—å†™å…¥æˆåŠŸï¼Œå­—ç¬¦æ•°: {result}', 'info')
                            else:
                                self._add_log(f'[DEBUG] æ–‡æœ¬å—å†™å…¥å¤±è´¥ï¼Œè¿”å›å€¼: {result}', 'error')

                        except Exception as text_err:
                            # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•å…¶ä»–ä¸­æ–‡å­—ä½“åç§°
                            font_names = ["china-t", "china-ss", "cjk", "song"]
                            font_success = False

                            for font_name in font_names:
                                try:
                                    result = new_page.insert_textbox(
                                        text_rect,
                                        translated_text,
                                        fontsize=11,
                                        fontname=font_name,
                                        color=(0, 0, 0),
                                        align=0
                                    )
                                    if result >= 0:
                                        success_count += 1
                                        total_written += 1
                                        self._add_log(f'[DEBUG] ä½¿ç”¨å­—ä½“ {font_name} å†™å…¥æˆåŠŸ', 'info')
                                        font_success = True
                                        break
                                except:
                                    continue

                            if not font_success:
                                self._add_log(f'[DEBUG] æ‰€æœ‰å­—ä½“å°è¯•å¤±è´¥: {str(text_err)[:50]}', 'error')

                    except Exception as e:
                        print(f'Insert textbox error on page {page_num + 1}: {e}')
                        import traceback
                        traceback.print_exc()
                        self._add_log(f'âš ï¸ ç¬¬{page_num + 1}é¡µå—{idx + 1}å†™å…¥å¤±è´¥: {str(e)[:100]}', 'error')

                if page_num < 3 or page_num >= total_pages - 3:
                    self._add_log(f'ç¬¬ {page_num + 1} é¡µå®Œæˆ: æˆåŠŸå†™å…¥ {success_count}/{len(page_translations)} ä¸ªæ–‡æœ¬å—', 'success')

                # æ›´æ–°è¿›åº¦
                elapsed_time = time.time() - translation_start_time
                self._update_progress(
                    page_num + 1,
                    total_pages,
                    f'å·²å†™å…¥ {page_num + 1}/{total_pages} é¡µ',
                    elapsed_time=elapsed_time,
                    estimated_remaining=0
                )

            # å…³é—­åŸæ–‡æ¡£
            doc.close()

            self._add_log(f'âœ“ æ€»å…±å†™å…¥ {total_written} ä¸ªæ–‡æœ¬å—åˆ°PDF', 'success')

            if total_written == 0:
                self._add_log('âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰ä»»ä½•æ–‡æœ¬è¢«å†™å…¥ï¼è¯·æ£€æŸ¥ä¸Šé¢çš„æ—¥å¿—', 'error')

            # ä¿å­˜ç¿»è¯‘åçš„PDF
            self._add_log(f'æ­£åœ¨ä¿å­˜ç¿»è¯‘åçš„PDFåˆ°: {output_path}', 'info')

            # æ£€æŸ¥è¾“å‡ºè·¯å¾„
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                self._add_log(f'âš ï¸ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}', 'error')

            # ä¿å­˜æ–°æ–‡æ¡£ï¼ˆåŒ…å«ç¿»è¯‘åçš„æ–‡æœ¬å’ŒåŸå›¾ï¼‰
            new_doc.save(output_path)
            new_doc.close()

            # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                self._add_log(f'âœ“ æ–‡ä»¶ä¿å­˜æˆåŠŸï¼å¤§å°: {file_size / 1024:.2f} KB', 'success')
            else:
                self._add_log('âœ— æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼æ–‡ä»¶ä¸å­˜åœ¨', 'error')

            # æœ€ç»ˆç»Ÿè®¡
            total_cost = self._calculate_cost()
            self._add_log(f'\n========== ç¿»è¯‘å®Œæˆ ==========', 'success')
            self._add_log(f'è¾“å…¥tokens: {self.input_tokens:,}', 'info')
            self._add_log(f'è¾“å‡ºtokens: {self.output_tokens:,}', 'info')
            self._add_log(f'é¢„ä¼°è´¹ç”¨: ${total_cost:.4f} USD', 'info')

            self._update_progress(total_pages, total_pages, 'ç¿»è¯‘å®Œæˆï¼')
            print(f'Translation completed: {output_path}')

        except Exception as e:
            print(f'Fatal error in translate_pdf: {e}')
            import traceback
            traceback.print_exc()
            self._add_log(f'ç¿»è¯‘è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}', 'error')
            if doc:
                doc.close()
            raise

    def translate_pdf_to_text(self, input_path, output_path, source_lang='auto', target_lang='zh', concurrency=4):
        """æå–PDFæ–‡æœ¬ï¼Œç¿»è¯‘æˆæŒ‡å®šè¯­è¨€ï¼Œç”ŸæˆTXTæ–‡ä»¶"""
        import concurrent.futures
        import threading

        total_start_time = time.time()

        self._add_log('========== å¼€å§‹æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡ ==========', 'info')
        self._add_log(f'è¾“å…¥æ–‡ä»¶: {input_path}', 'info')
        self._add_log(f'è¾“å‡ºæ–‡ä»¶: {output_path}', 'info')
        self._add_log(f'æºè¯­è¨€: {source_lang}', 'info')
        self._add_log(f'ç›®æ ‡è¯­è¨€: {target_lang}', 'info')
        self._add_log(f'å¹¶å‘çº¿ç¨‹æ•°: {concurrency}', 'info')

        # è¯­è¨€ä»£ç æ˜ å°„
        lang_map = {
            'zh': 'zh-cn',
            'en': 'en',
            'ja': 'ja',
            'ko': 'ko',
            'fr': 'fr',
            'de': 'de',
            'es': 'es',
            'ru': 'ru',
            'ar': 'ar'
        }

        normalized_source = lang_map.get(source_lang, source_lang)
        normalized_target = lang_map.get(target_lang, target_lang)

        # æ‰“å¼€PDFå¹¶æå–æ–‡æœ¬
        extract_start = time.time()
        self._add_log('æ­£åœ¨æå–PDFæ–‡æœ¬...', 'info')
        doc = fitz.open(input_path)
        total_pages = len(doc)

        all_text_blocks = []
        current_position = 0

        for page_num in range(total_pages):
            self._check_cancelled()
            page = doc[page_num]
            text = page.get_text()

            if text and text.strip():
                all_text_blocks.append({
                    'page_num': page_num,
                    'text': text.strip()
                })
                current_position += len(text)

        doc.close()
        extract_time = time.time() - extract_start
        self._add_log(f'âœ“ æå–åˆ° {len(all_text_blocks)} é¡µæ–‡æœ¬ï¼Œå…± {current_position} ä¸ªå­—ç¬¦ (è€—æ—¶: {extract_time:.1f}ç§’)', 'info')

        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        full_text = '\n\n'.join([block['text'] for block in all_text_blocks])
        self._add_log(f'åˆå¹¶åæ€»å­—ç¬¦æ•°: {len(full_text)}', 'info')

        # å°†æ–‡æœ¬åˆ†å‰²æˆå—è¿›è¡Œç¿»è¯‘ï¼ˆæ¯å—çº¦4000å­—ç¬¦ï¼Œå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡ï¼‰
        chunk_size = 4000
        text_chunks = []
        for i in range(0, len(full_text), chunk_size):
            chunk = full_text[i:i + chunk_size]
            text_chunks.append(chunk)

        self._add_log(f'åˆ†æˆ {len(text_chunks)} ä¸ªæ–‡æœ¬å—è¿›è¡Œç¿»è¯‘ (æ¯å—çº¦{chunk_size}å­—ç¬¦)', 'info')

        # å¹¶å‘ç¿»è¯‘
        translation_start_time = time.time()
        completed_count = [0]
        lock = threading.Lock()
        results = {}
        api_times = []  # è®°å½•æ¯æ¬¡APIè°ƒç”¨è€—æ—¶

        def translate_chunk(chunk_info):
            """ç¿»è¯‘å•ä¸ªæ–‡æœ¬å—"""
            chunk_idx = chunk_info['index']
            text = chunk_info['text']

            try:
                self._check_cancelled()

                # è®°å½•è¾“å…¥token
                input_tokens = self._estimate_tokens(text)

                # ç¿»è¯‘ - è®¡æ—¶
                api_start = time.time()
                if self.api_type == 'google':
                    translated = GoogleTranslator(source=normalized_source, target=normalized_target).translate(text)
                else:
                    translated = self._translate_text(text, source_lang, target_lang)
                api_time = time.time() - api_start
                api_times.append(api_time)

                # è®°å½•è¾“å‡ºtoken
                output_tokens = self._estimate_tokens(translated)

                # æ˜¾ç¤ºå½“å‰ç¿»è¯‘å†…å®¹ï¼ˆåŒ…å«åŸæ–‡å’Œè¯‘æ–‡ï¼‰
                display_text = text[:100] + '...' if len(text) > 100 else text
                display_translated = translated[:100] + '...' if len(translated) > 100 else translated
                # å‘é€åŸæ–‡æ—¥å¿—
                self._add_log(f'[åŸæ–‡ {chunk_idx + 1}/{len(text_chunks)}] {display_text}', 'info')
                # å‘é€è¯‘æ–‡æ—¥å¿—
                self._add_log(f'[è¯‘æ–‡ {chunk_idx + 1}/{len(text_chunks)}] {display_translated} (è€—æ—¶: {api_time:.1f}s)', 'success')

                with lock:
                    self.input_tokens += input_tokens
                    self.output_tokens += output_tokens
                    completed_count[0] += 1

                    # æ›´æ–°è¿›åº¦
                    elapsed_time = time.time() - translation_start_time
                    current = completed_count[0]
                    self._update_progress(
                        current,
                        len(text_chunks),
                        f'å·²ç¿»è¯‘ {current}/{len(text_chunks)} ä¸ªæ–‡æœ¬å—...',
                        elapsed_time=elapsed_time,
                        estimated_remaining=0
                    )

                    # è®°å½•ç¿»è¯‘ç»“æœ
                    results[chunk_idx] = translated

                return chunk_idx, translated

            except Exception as e:
                print(f'Translation error for chunk {chunk_idx}: {e}')
                with lock:
                    completed_count[0] += 1
                return chunk_idx, text  # å¤±è´¥æ—¶è¿”å›åŸæ–‡

        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        chunk_tasks = [{'index': i, 'text': text_chunks[i]} for i in range(len(text_chunks))]

        self._add_log('å¼€å§‹ç¿»è¯‘...', 'info')

        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = {executor.submit(translate_chunk, chunk): chunk for chunk in chunk_tasks}

            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f'Future error: {e}')

        translation_time = time.time() - translation_start_time
        self._add_log('âœ“ æ‰€æœ‰æ–‡æœ¬å—ç¿»è¯‘å®Œæˆ', 'success')

        # è¾“å‡ºè€—æ—¶ç»Ÿè®¡
        if api_times:
            avg_api_time = sum(api_times) / len(api_times)
            max_api_time = max(api_times)
            min_api_time = min(api_times)
            total_api_time = sum(api_times)
            self._add_log(f'ğŸ“Š APIè€—æ—¶ç»Ÿè®¡:', 'info')
            self._add_log(f'  - æ€»ç¿»è¯‘æ—¶é—´: {translation_time:.1f}ç§’', 'info')
            self._add_log(f'  - APIè°ƒç”¨æ€»è€—æ—¶: {total_api_time:.1f}ç§’', 'info')
            self._add_log(f'  - å•æ¬¡APIå¹³å‡: {avg_api_time:.1f}ç§’', 'info')
            self._add_log(f'  - å•æ¬¡APIæœ€å¿«: {min_api_time:.1f}ç§’', 'info')
            self._add_log(f'  - å•æ¬¡APIæœ€æ…¢: {max_api_time:.1f}ç§’', 'info')

        # æŒ‰é¡ºåºç»„åˆç¿»è¯‘ç»“æœ
        translated_text = '\n\n'.join([results[i] for i in range(len(text_chunks))])

        # ä¿å­˜ä¸ºTXTæ–‡ä»¶
        save_start = time.time()
        self._add_log(f'æ­£åœ¨ä¿å­˜ç¿»è¯‘åçš„æ–‡æœ¬åˆ°: {output_path}', 'info')

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(translated_text)

        save_time = time.time() - save_start

        total_time = time.time() - total_start_time
        self._add_log('âœ“ æ–‡æœ¬ç¿»è¯‘å®Œæˆï¼', 'success')
        self._add_log(f'ğŸ“Š å„é˜¶æ®µè€—æ—¶:', 'info')
        self._add_log(f'  - PDFæ–‡æœ¬æå–: {extract_time:.1f}ç§’', 'info')
        self._add_log(f'  - ç¿»è¯‘APIè°ƒç”¨: {translation_time:.1f}ç§’', 'info')
        self._add_log(f'  - æ–‡ä»¶ä¿å­˜: {save_time:.1f}ç§’', 'info')
        self._add_log(f'  - æ€»è€—æ—¶: {total_time:.1f}ç§’', 'info')
        self._add_log(f'è¾“å…¥tokens: {self.input_tokens:,}', 'info')
        self._add_log(f'è¾“å‡ºtokens: {self.output_tokens:,}', 'info')
        self._add_log(f'é¢„ä¼°è´¹ç”¨: ${self._calculate_cost():.4f} USD', 'info')
        self._add_log('=' * 40, 'info')
